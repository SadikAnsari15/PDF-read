{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BankStatement Analysis with Gen AI\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction\n",
    "2. Examples\n",
    "3. References and Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Generative AI can be utilized in various ways for writing data engineering code, specifically for creating efficient and accurate data pipelines:\n",
    "\n",
    "1. Code Generation: Automatically generating data engineering scripts based on high-level descriptions of tasks.\n",
    "2. Optimization: Improving existing data engineering code based on performance feedback and best practices.\n",
    "3. Schema Understanding: Interpreting data schemas to inform code generation and optimization.\n",
    "4. Error Detection and Correction: Identifying and fixing errors in data engineering code through automated analysis.\n",
    "5. Code Translation: Converting code between different programming languages and frameworks used in data engineering.\n",
    "6. Complex Workflow Creation: Generating complex data workflows and pipelines based on user requirements.\n",
    "7. Result Interpretation: Translating data processing results into human-readable reports and summaries.\n",
    "8. Data Quality Checks: Generating code for validating data quality and consistency in pipelines.\n",
    "9. Documentation Generation: Creating detailed documentation for data engineering code and workflows automatically.\n",
    "\n",
    "Using Gen AI for this task offers several benefits:\n",
    "\n",
    "- Increased productivity and efficiency for data engineers\n",
    "- Faster development and deployment of data pipelines\n",
    "- Reduced errors in code\n",
    "- Improved maintainability and readability of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.45.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from openai) (2.7.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting pdf2image\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdf2image) (10.3.0)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pdf2image\n",
      "Successfully installed pdf2image-1.17.0\n",
      "Collecting poppler-utils\n",
      "  Downloading poppler_utils-0.1.0-py3-none-any.whl.metadata (883 bytes)\n",
      "Requirement already satisfied: Click>=7.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from poppler-utils) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from Click>=7.0->poppler-utils) (0.4.6)\n",
      "Downloading poppler_utils-0.1.0-py3-none-any.whl (9.2 kB)\n",
      "Installing collected packages: poppler-utils\n",
      "Successfully installed poppler-utils-0.1.0\n",
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.24.10-cp312-none-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.24.10 (from PyMuPDF)\n",
      "  Downloading PyMuPDFb-1.24.10-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Downloading PyMuPDF-1.24.10-cp312-none-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.2/3.2 MB 31.6 MB/s eta 0:00:00\n",
      "Downloading PyMuPDFb-1.24.10-py3-none-win_amd64.whl (13.2 MB)\n",
      "   ---------------------------------------- 0.0/13.2 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 8.7/13.2 MB 44.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.2 MB 43.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.2 MB 43.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.2 MB 43.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.2 MB 43.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.2/13.2 MB 12.2 MB/s eta 0:00:00\n",
      "Installing collected packages: PyMuPDFb, PyMuPDF\n",
      "Successfully installed PyMuPDF-1.24.10 PyMuPDFb-1.24.10\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (10.3.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from pytesseract) (24.0)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install pdf2image\n",
    "!pip install poppler-utils\n",
    "!pip install PyMuPDF\n",
    "!pip install pytesseract Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI \n",
    "import os \n",
    "import requests \n",
    "from pdf2image import convert_from_bytes \n",
    "from PIL import Image \n",
    "import base64 \n",
    "import json\n",
    "import pytesseract\n",
    "from io import BytesIO \n",
    "import pandas as pd\n",
    "\n",
    "def clean(dict_variable):\n",
    "    return next(iter(dict_variable.values()))\n",
    "    \n",
    "client = OpenAI(api_key='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')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 2. Fetch from PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # For Windows\n",
    "\n",
    "# OCR function to extract text from image\n",
    "# Function to extract text from an image\n",
    "def extract_text_from_image(image):\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting text: {str(e)}\"\n",
    "# URL of the image\n",
    "# image_url = \"https://crm.autocarloan.co.uk/images/66e192f2f12a94a822acbbc1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_bytes\n",
    "import io\n",
    "import os\n",
    "\n",
    "# Function to read PDF file as bytes\n",
    "def read_pdf_as_bytes(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_bytes = file.read()\n",
    "        return pdf_bytes\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return None\n",
    "        \n",
    "# Function to convert PDF bytes to images\n",
    "def convert_pdf_bytes_to_images(pdf_bytes, poppler_path=None):\n",
    "    try:\n",
    "        # Convert PDF bytes to images using pdf2image\n",
    "        images = convert_from_bytes(pdf_bytes, poppler_path=poppler_path)\n",
    "        return images\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting PDF bytes to images: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main function\n",
    "def main(pdf_path):\n",
    "    poppler_path = r'C:\\Program Files\\poppler-24.07.0\\Library\\bin'  # Adjust this to the path where poppler is installed (Windows only)\n",
    "    \n",
    "    # Step 1: Read PDF file as bytes\n",
    "    pdf_bytes = read_pdf_as_bytes(pdf_path)\n",
    "\n",
    "    if pdf_bytes is None:\n",
    "        print(\"Failed to read the PDF file.\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Convert PDF bytes to images\n",
    "    images = convert_pdf_bytes_to_images(pdf_bytes, poppler_path=poppler_path)\n",
    "\n",
    "    if images is None:\n",
    "        print(\"Failed to convert PDF to images.\")\n",
    "        return\n",
    "\n",
    "      # Step 3: Extract text from each image and save images (optional)\n",
    "    all_extracted_text = \"\"  # Initialize variable to store all extracted text\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = f'output_image_page_{i + 1}.png'\n",
    "        image.save(image_path, 'PNG')\n",
    "        # print(f\"Saved {image_path}\")\n",
    "\n",
    "        # Extract text from the image\n",
    "        extracted_text = extract_text_from_image(image)\n",
    "        all_extracted_text += extracted_text + \"\\n\\n\"  # Accumulate extracted text with spacing between pages\n",
    "\n",
    "    # Step 4: Print or store all extracted text\n",
    "    # print(\"Extracted Text from PDF:\")\n",
    "    # print(all_extracted_text)\n",
    "\n",
    "    # Optionally, you can save the extracted text to a file\n",
    "    with open(f'{os.path.splitext(pdf_path)[0]}_extracted_text.txt', 'w', encoding='utf-8') as text_file:\n",
    "        text_file.write(all_extracted_text)\n",
    "    return all_extracted_text\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = 'Bank of Scotland 2.pdf'  # Replace with your local PDF file path\n",
    "    wholetext = main(pdf_path)\n",
    "    # print(wholetext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to request the model and return JSON output\n",
    "def extract_json_from_model(client, model, wholetext_chunk, is_continuation=False):\n",
    "    system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in JSON format.\"}\n",
    "    \n",
    "    # If it's a continuation, modify the user prompt to ask for the next part\n",
    "    if is_continuation:\n",
    "        user_message = {\"role\": \"user\", \"content\": \"Please continue from where the previous response left off and provide the next JSON chunk.\"}\n",
    "    else:\n",
    "        user_message = {\"role\": \"user\", \"content\": f\"Extract the statement's line items: Date, Description, Type, Money_In, Money_Out, and Balance from the following text: {wholetext_chunk}\"}\n",
    "\n",
    "    # Send request to GPT-4o-mini model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[system_message, user_message],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    # print(response.choices[0].message.content)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Function to clean and parse the JSON response\n",
    "def parse_json_response(json_str):\n",
    "    try:\n",
    "        if json_str.startswith('```json'):\n",
    "            json_str = json_str.lstrip('```json').rstrip('```')\n",
    "        parsed_json = json.loads(json_str)\n",
    "        # print(\"Parsed JSON:\", parsed_json)  # Debug print\n",
    "        return parsed_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decoding failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract JSON data repeatedly until all chunks are processed\n",
    "def extract_large_json(client, model, wholetext, chunk_size=5000):\n",
    "    all_results = []\n",
    "    text_length = len(wholetext)\n",
    "    current_position = 0\n",
    "\n",
    "    # Loop through chunks of the input text\n",
    "    while current_position < text_length:\n",
    "        chunk_end = min(current_position + chunk_size, text_length)\n",
    "        text_chunk = wholetext[current_position:chunk_end]\n",
    "        \n",
    "        # Extract JSON from the current chunk\n",
    "        response_chunk = extract_json_from_model(client, model, text_chunk)  # Corrected this line\n",
    "        \n",
    "        # Parse the JSON from the response\n",
    "        result = parse_json_response(response_chunk)\n",
    "        if result is not None:\n",
    "            all_results.append(result)\n",
    "\n",
    "        # If response indicates end of content or no valid JSON, break\n",
    "        if response_chunk.lstrip('```json').rstrip('```').endswith(\"}\"):\n",
    "            break\n",
    "\n",
    "        # Move to the next chunk\n",
    "        current_position = chunk_end\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def extract_items(results):\n",
    "    if not all(isinstance(item, dict) for item in results):\n",
    "        print(\"Error: Results should be a list of dictionaries.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame\n",
    "    # Initialize lists to store extracted data\n",
    "    dates = []\n",
    "    descriptions = []\n",
    "    types = []\n",
    "    money_in = []\n",
    "    money_out = []\n",
    "    balances = []\n",
    "\n",
    "    # Extract items from each result\n",
    "    for result in results:\n",
    "        transactions = result.get(\"transactions\", [])\n",
    "        for transaction in transactions:\n",
    "            dates.append(transaction.get(\"Date\", \"\"))\n",
    "            descriptions.append(transaction.get(\"Description\", \"\"))\n",
    "            types.append(transaction.get(\"Type\", \"\"))\n",
    "            money_in.append(transaction.get(\"Money_In\", 0.0))\n",
    "            money_out.append(transaction.get(\"Money_Out\", 0.0))\n",
    "            balances.append(transaction.get(\"Balance\", 0.0))\n",
    "\n",
    "\n",
    "    # Create a DataFrame for better visualization (optional)\n",
    "    df = pd.DataFrame({\n",
    "        \"Date\": dates,\n",
    "        \"Description\": descriptions,\n",
    "        \"Type\": types,\n",
    "        \"Money_In\": money_in,\n",
    "        \"Money_Out\": money_out,\n",
    "        \"Balance\": balances\n",
    "    })\n",
    "\n",
    "    return df\n",
    "# Function to consolidate and clean results (if necessary)\n",
    "def clean_consolidate_results(results):\n",
    "    cleaned_results = []\n",
    "    \n",
    "    for result in results:\n",
    "        if isinstance(result, str):\n",
    "            try:\n",
    "                parsed_result = json.loads(result)\n",
    "                if isinstance(parsed_result, list):\n",
    "                    cleaned_results.extend(parsed_result)\n",
    "                elif isinstance(parsed_result, dict):\n",
    "                    cleaned_results.append(parsed_result)\n",
    "                else:\n",
    "                    print(\"Unexpected JSON structure:\", parsed_result)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decoding failed: {e}\")\n",
    "        elif isinstance(result, dict):\n",
    "            cleaned_results.append(result)\n",
    "        else:\n",
    "            print(\"Unexpected result type:\", type(result))\n",
    "    \n",
    "    return cleaned_results\n",
    "\n",
    "# Main function\n",
    "def main(client, model, pdf_text):\n",
    "    # Step 1: Extract large JSON by processing the text in chunks\n",
    "    extracted_results = extract_large_json(client, model, pdf_text, chunk_size=5000)\n",
    "    \n",
    "    if extracted_results:\n",
    "        # Step 2: Clean and consolidate the extracted results\n",
    "        consolidated_results = clean_consolidate_results(extracted_results)\n",
    "\n",
    "       # Step 3: Extract items from the consolidated results\n",
    "        finalresp = extract_items(consolidated_results)\n",
    "        return finalresp\n",
    "    else:\n",
    "        print(\"No valid JSON data extracted.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming `client` is the OpenAI client and `pdf_text` contains the extracted text from the PDF\n",
    "    model = \"gpt-4o-mini\"\n",
    "    # print(wholetext)\n",
    "    dfall = main(client, model, wholetext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date         Description Type  Money_In  Money_Out  Balance\n",
      "0   2021-07-01   TESCO PAY AT PUMP  DEB      0.00      40.00  -311.39\n",
      "1   2021-07-01         LIDL GB AYR  DEB      0.00       3.65  -299.54\n",
      "2   2021-07-01          GREGGS PLC  DEB      0.00       5.90  -320.94\n",
      "3   2021-07-01   PPOINT_*WOODFIELD  DEB      0.00       7.36  -328.30\n",
      "4   2021-07-01      TESCO PFS 4143  DEB      0.00      11.45  -339.75\n",
      "..         ...                 ...  ...       ...        ...      ...\n",
      "172 2021-07-30           J THOMSON  TFR     90.00       0.00  -318.97\n",
      "173 2021-07-30           J THOMSON  TFR   1078.58       0.00  -318.97\n",
      "174 2021-07-30  PAYPAL *APPLE.COM/  DEB      7.99       0.00  -318.97\n",
      "175 2021-07-30           J THOMSON  TFR     78.00       0.00  -318.97\n",
      "176 2021-07-30        DAILY OD INT  CHG      0.33       0.00  -318.97\n",
      "\n",
      "[177 rows x 6 columns]\n",
      "  YearMonth  DisposableAmount\n",
      "0   2021-07           9067.57\n"
     ]
    }
   ],
   "source": [
    "dfall['Date'] = pd.to_datetime(dfall['Date'],format='%d %b %y')\n",
    "\n",
    "print(dfall)\n",
    "dfall.to_csv('output.csv', index=False)\n",
    "\n",
    "# Extract the year and month for grouping\n",
    "dfall['YearMonth'] = dfall['Date'].dt.to_period('M')\n",
    "\n",
    "# Calculate disposable amount (Credit - Debit) for each month\n",
    "monthly_disposable = dfall.groupby('YearMonth').apply(lambda x: x['Money_In'].sum() - x['Money_Out'].sum(), include_groups=False)\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "monthly_disposable_df = monthly_disposable.reset_index(name='DisposableAmount')\n",
    "\n",
    "print(monthly_disposable_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To arrive at a lending decision based on the provided criteria, we need to analyze the `DisposableAmount` for the given months.\n",
      "\n",
      "From the data provided:\n",
      "\n",
      "- July 2021: 9067.57\n",
      "\n",
      "Since we do not have data for the past three months (June 2021 and May 2021 are missing):\n",
      "\n",
      "1. **Checking for last three months:**\n",
      "   - We can only see the amount for July 2021, which is 9067.57. Since we do not have values for the previous two months, we cannot determine if the disposable amount was above or below 1000 for three consecutive months.\n",
      "\n",
      "2. **Reviewing the criteria:**\n",
      "   - Approve only if the disposable amount is more than 1000 for the last three months: **Cannot determine, as data is incomplete.**\n",
      "   - Decline if it's less than 1000 for all the months: **Not applicable here, as we only have one month’s data.**\n",
      "   - Deferred to Underwriter if disposable amount is more than 1000 for 1 or more months: **Here it applies**, as 9067.57 is greater than 1000.\n",
      "\n",
      "Therefore, based on the available information, the lending decision is: **Deferred to Underwriter**.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Arrive a lending decision with status Approved,Declined or Deferred to Underwriter by reviewing the below data\n",
    "{monthly_disposable_df} . Approve only if disposable Amount is more than 1000 for last 3 months, Decline if its less 1000 for all the months, Deferred to Underwriter if disposable amount is more than 1000 for 1 or more months.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    ")\n",
    "\n",
    "result = response.choices[0].message.content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
